Title: Lumi — Detailed Daily Stats & Health
Date: 2026-01-29
Author: Automated runtime auditor

Executive summary
- This document captures Lumi's runtime and knowledge health as of 2026-01-29. It includes KB/training/memory status, runtime agent health, self-learn behavior, audit summaries, and recommended operational adjustments.

High-level health
- Build & runtime: application TypeScript build is clean and runnable. Smoke self‑learn tests executed successfully and confirmed LLM connectivity in the developer environment.
- Privacy posture: active redaction and backup-archival completed; repository no longer contains in-repo forensic backups.

Knowledge & Training (state)
- Canonical KB: `training/lumi_knowledge.json` — populated (~130+ QA entries observed during audit). Entries include seed persona items and historical learned QA. Current content was inspected and validated as suitable for beta.
- Training logs: `training/training.jsonl` — contains audit/training events and a small set of historical/example entries. User confirmed these entries are non-sensitive sample data.
- Embeddings & deduplication: codebase exposes hooks in `KnowledgeProcessor` for semantic deduplication; no production embedding provider is finalized. Current dedupe behavior is basic/exact and relies on text matching or simple heuristics.

Memory & Signals
- Memory store: file-backed JSONL store under `userData/` (append-only pattern). Used for session signals, curator staging, and validation/audit logs.
- Audit & validation counters: write-ahead audit lines are appended by `KnowledgeProcessor` and `SignalProcessor`. Smoke tests incremented audit counters confirming persistence and append semantics.
- Persistence durability: operations are synchronous file appends with occasional batch writes via helper scripts; no external DB required for Phase 1.

Agent & Self-Learn
- DeepLearningAgent: present and instantiated by `src/main.ts`. It scans allowed file paths and generates suggestions via the Ollama client.
- Start/stop: UI exposes a one-click self-learn toggle which prefers IPC control but falls back to persisted config toggles when IPC is unavailable.
- Rate limiting: deep scan mode configured conservatively (default ~6 files/min deep mode) to avoid heavy resource usage. This is configurable in the agent.
- Suggestion pipeline: files → extractor(LM) → validator → curator staging → curator approve → canonical KB write. Curator UI present for manual review before promotion.

LLM Integration
- Ollama client integration present and used for extraction/synthesis. Full extraction features require a running local Ollama server (developer confirmed Ollama reachable during smoke tests).
- Streaming and generation: ollama client supports streaming and is wired into `KnowledgeProcessor` for extraction; fallbacks exist when Ollama is unavailable (KB-only responses).

Operational metrics (qualitative)
- Knowledge coverage: moderate — the KB (~130 entries) covers persona & common assistant behaviors; coverage for edge developer tasks needs growth via curated onboarding sessions.
- Training churn: low — training/log files show historical entries and occasional curator-promoted items; no frequent or runaway learning observed during the smoke test.
- Memory growth: bounded — local conversation history is capped (e.g., last 2000 entries in renderer-local store). File-backed signal stores are append-only and require periodic compaction/archival for long-term operation.

Security & privacy status
- Redaction: performed and backups archived off-repo.
- Active findings: none on repo surface after remediation; training files were reviewed and confirmed non-sensitive.

Developer notes & actionable items
- Finalize embedding provider and semantic deduplication pipeline to reduce KB duplication and improve retrieval quality.
- Add unit/integration tests for `KnowledgeProcessor` to assert redaction and write behavior across upgrade scenarios.
- Add a periodic compaction job for JSONL memory stores to keep disk footprint predictable.
- Consider a lightweight quota/rotation for audit logs and staging archives to prevent unbounded growth.

Appendix: quick health checklist
- Build: `npx tsc -p tsconfig.json` ✅
- Privacy audit: `node scripts/privacy_audit.js` ✅ (repo surface clean)
- Smoke self-learn: `node scripts/smoke_selflearn_test.js` ✅ (Ollama reachable in dev)
- CI prelaunch checks: `npm run prelaunch:checks` ✅ (local run OK)

Notes on archived backups
- Forensics/backups were archived to a local, non-repository location per the team's decision. Do not include the local absolute path in public-facing logs; retrieval instructions should be communicated separately via an internal channel.

End of Lumi daily stats (2026-01-29).
# Lumi Daily Log — 2026-01-29

Highlights:
- Improved session memory controls: user-selectable memory limit (default 50), summarization of older entries, and auto-promote options for archiving.
- The curator UI refreshed to show promoted archives and includes a Refresh control; archive files now live under `userData/sessions`.
- Added simulator improvements to exercise session archiving and promotion end-to-end.

Developer notes:
- Summarization uses the `think()` engine with `kbFirst` to reduce hallucination risk; it's intended to produce concise bullet summaries of older conversation history.
- Be mindful of token limits when switching to `Full` session memory. The UI now warns users on selection.

Commands used during testing:

```powershell
node test/simulate_sessions.js
$env:AUTO_PROMOTE=1; node test/simulate_sessions.js
```

Potential follow-ups:
- Add a token-meter (exact) and a Settings tab for adjusting summarization and warning thresholds.
