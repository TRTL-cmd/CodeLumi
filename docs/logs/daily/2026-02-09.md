# Daily Log - 2026-02-09

## Overview
Work focused on stabilizing Lumi's code sandbox, improving personality responsiveness, and reducing fallbacks during LLM calls. The app UI was consolidated so there is a single code sandbox/editor flow, and the sandbox now consistently receives code from assistant responses. Several reliability fixes were added to prevent timeouts and ensure code requests get higher quality and fuller responses.

## Changes and Additions

### 1) Personality System: Behavior and Quality Tiers
- Upgraded sentiment analysis and rapport tracking so response quality degrades or improves based on user behavior.
- Added weighted keyword tiers, phrase detection, negation handling, and ALL-CAPS intensity.
- Added response quality tiers (0-4) that affect the system prompt and refusal behavior.
- Updated IPC handling so requests carry personality tier into the model prompt.

Files:
- src/core/personality/PersonalityEngine.ts
- src/main.ts
- src/core/brain/index.ts

### 2) Code Sandbox: Single Source of Truth
- Replaced the old textarea UI with Monaco-based editor integration in the main UI.
- Merged code responses into the existing sandbox editor instead of replacing it.
- Added code context injection so follow-up edits build on existing code.
- Added a single sandbox API (window.codeSandbox) so any UI can route code into the main editor.
- Removed the secondary React sandbox UI (AutoCodeBox editor) and made it only forward code to the main sandbox.
- Ensured the code panel is draggable only from its header to allow text selection in the editor.

Files:
- index.html
- src/components/AutoCodeBox.tsx

### 3) Sandbox Controls and Behavior
- Added a clear button for the sandbox to reset code.
- Removed pause/resume behavior to avoid blocking code merges.
- Auto-merging is now always on.

Files:
- index.html

### 4) LLM Reliability and Code Quality
- Added code-quality system prompt for code requests to encourage complete, working outputs.
- Increased timeouts and token budgets for code-heavy prompts in the renderer.
- Added abort detection and retry logic in the brain layer to avoid fallback when Ollama aborts.

Files:
- src/core/brain/index.ts
- index.html

### 5) Security/Quarantine Adjustments
- Stopped quarantining content that is only flagged as code-like so assistant code outputs are not blocked.

Files:
- src/main.ts

### 6) CSP and Monaco Workers
- Updated CSP to allow Monaco web workers (worker-src with blob/data).

Files:
- index.html

## Notable Behavior Changes
- There is now only one code sandbox/editor. All assistant code funnels into that editor.
- Code context is always included in model prompts when present.
- LLM requests for code have a larger timeout and retry once if aborted.
- Clear now resets the sandbox without disabling auto-merge.

## Known Issues/Observations
- Ollama can still abort on very large generations if the model is slow or overloaded. The new retry and higher timeout reduce this, but heavy requests may still fail depending on system load.
- If a coder model is not available, some code outputs may be weaker than desired. Consider switching to a code-focused model for larger tasks.

## Tests/Verification Performed
- TypeScript checks with npx tsc --noEmit (no errors reported).
- Vite build (npx vite build) succeeded with expected chunk size warnings.

## Files Touched (Summary)
- index.html
- src/core/personality/PersonalityEngine.ts
- src/main.ts
- src/core/brain/index.ts
- src/preload.ts
- src/components/AutoCodeBox.tsx

## Next Steps (Optional)
- Add automatic model switching for code tasks (e.g., qwen2.5-coder) when available.
- Add a single code history/snapshot view tied directly to the sandbox session.
- Add a compact status chip showing active code language in the sandbox header.
